{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../../libraries/pvae/\")\n",
    "from pvae.manifolds.poincareball import PoincareBall\n",
    "from pvae.manifolds.euclidean import Euclidean\n",
    "from pvae.models.architectures import EncWrapped, DecWrapped\n",
    "from pvae.distributions.wrapped_normal import WrappedNormal\n",
    "from pvae.distributions.riemannian_normal import RiemannianNormal\n",
    "from pvae.ops.manifold_layers import GeodesicLayer\n",
    "from pvae.objectives import vae_objective\n",
    "from torch.distributions.normal import Normal\n",
    "sys.path.append(\"../../libraries/\")\n",
    "from HypHC.optim.radam import RAdam\n",
    "from HypHC.utils.poincare import project\n",
    "from HypHC.utils.visualization import plot_tree_from_leaves\n",
    "from HypHC.utils.linkage import nn_merge_uf_fast_np, sl_from_embeddings\n",
    "from HypHC.utils.metrics import dasgupta_cost\n",
    "sys.path.append(\"../hyperLAI\")\n",
    "import math\n",
    "import torch\n",
    "import joblib\n",
    "from torch import nn\n",
    "import networkx as nx\n",
    "import sknetwork\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from models.hyperbolic_hc_loss import HyperbolicHCLoss\n",
    "from models.encoder_decoder_architectures import *\n",
    "from models.vae_model import vae_model\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader, Subset\n",
    "from torch.optim import Adam\n",
    "from utils.sim_funcs import sim_func_dict\n",
    "from utils.model_utils import *\n",
    "from features.hyperLAIdataset import HyperLoader\n",
    "from models.fc_model import fc_model\n",
    "\n",
    "enc_dec_dict = {\"fc_wrapped_encoder\": fc_wrapped_encoder, \"fc_wrapped_decoder\": fc_wrapped_decoder, \"fc_geodesic_decoder\": fc_geodesic_decoder,\n",
    "               \"fc_wrapped_decoder_rawvals\": fc_wrapped_decoder_rawvals} \n",
    "manifold_dict = {\"PoincareBall\": PoincareBall, \"Euclidean\": Euclidean}\n",
    "distribution_dict = {\"WrappedNormal\": WrappedNormal, \"Normal\": Normal, \"RiemannianNormal\": RiemannianNormal}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Obtain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = \"/scratch/users/patelas/hyperLAI/models/vae_models/full_pop_models/500000_threelayers_tempnegsix_shuffle_newearlystop/\"\n",
    "args = read_config(model_dir + \"vae_config.json\")\n",
    "indices = np.load(args[\"index_loc\"] + \"valid_indices.npy\")\n",
    "# indices = np.load(\"/scratch/users/patelas/hyperLAI/ancestry_training_splits/80_10_10/test_indices.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load validation dataset\n",
    "eval_dataset = HyperLoader(args[\"data_dir\"], indices, args[\"restrict_labels\"], args[\"chromosome\"])\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pca_model = joblib.load(args[\"output_dir\"] + \"pca_model.joblib\")\n",
    "# eval_snps_old = eval_dataset.snps.copy()\n",
    "# eval_dataset.snps = pca_model.transform(eval_dataset.snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define manifold and model\n",
    "manifold = manifold_dict[args[\"manifold\"]](args[\"embedding_size\"])\n",
    "enc_type, dec_type  = enc_dec_dict[args[\"enc_type\"]], enc_dec_dict[args[\"dec_type\"]]\n",
    "encoder = enc_type(manifold, eval_dataset[0][0].shape[-1], args[\"num_encoder_int_layers\"], \n",
    "                   args[\"encoder_int_layer_sizes\"], args[\"encoder_dropout_vals\"], args[\"embedding_size\"])\n",
    "decoder = dec_type(manifold, eval_dataset[0][0].shape[-1], args[\"num_decoder_int_layers\"], \n",
    "                   args[\"decoder_int_layer_sizes\"], args[\"decoder_dropout_vals\"], args[\"embedding_size\"])\n",
    "model = vae_model(encoder, decoder, manifold, distribution_dict[args[\"posterior_dist\"]],\n",
    "                  distribution_dict[args[\"prior_dist\"]], args[\"prior_mean\"], args[\"prior_std\"], args[\"temperature\"], args[\"init_size\"], args[\"min_scale\"], args[\"max_scale\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = torch.load(\"%smodel.pt\"%(model_dir))\n",
    "model.load_state_dict(model_info[\"model_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cff2fbbc0f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/home/users/patelas/hyperLAI/pretrained_models/vae_model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/share/software/user/open/py-pytorch/1.6.0_py36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/software/user/open/py-pytorch/1.6.0_py36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), \"/home/users/patelas/hyperLAI/pretrained_models/vae_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run model on validation data\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "snps, embeddings, recons, suppop_labels, pop_labels = [], [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, (snp_data, suppop, pop) in enumerate(eval_loader):\n",
    "        embs = model.embed(snp_data.float().to(device))\n",
    "        recon = model.generate(embs.to(device))\n",
    "        snps.append(snp_data.cpu())\n",
    "        embeddings.append(embs.cpu())\n",
    "        recons.append(recon.cpu())\n",
    "        suppop_labels.append(suppop)\n",
    "        pop_labels.append(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert to numpy arrays\n",
    "snps = torch.cat(snps).numpy()\n",
    "embeddings = torch.cat(embeddings).numpy()\n",
    "recons = torch.cat(recons).numpy()\n",
    "suppop_labels = torch.cat(suppop_labels).numpy()\n",
    "pop_labels = torch.cat(pop_labels).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Node Embeddings (Perhaps as PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weights_pca(embeddings, labels, annotations=None):\n",
    "    '''\n",
    "    Plot embeddings. Uncomment/comment as necessary depending on if you want raw embeddings or PCA\n",
    "    '''\n",
    "#     weights_pca = PCA().fit_transform(embeddings)\n",
    "#     scplot = sns.scatterplot(x=weights_pca[:,0], y=weights_pca[:,1], hue=labels)\n",
    "    label_dict = {\"EUR\": \"Europe\", \"EAS\": \"East Asia\", \"AMR\": \"Americas\", \"AFR\": \"Africa\",\"SAS\": \"South Asia\", \n",
    "                  \"OCE\": \"Oceania\", \"WAS\": \"West Asia\"}\n",
    "#     h_order = [\"EUR\", \"EAS\", \"AMR\", \"AFR\", \"SAS\", \"OCE\", \"WAS\"]\n",
    "    h_order = [\"Europe\", \"East Asia\", \"Americas\", \"Africa\", \"South Asia\", \"Oceania\", \"West Asia\"]\n",
    "    sns.set_style('white')\n",
    "    scplot = sns.scatterplot(x=embeddings[:,0], y=embeddings[:,1], hue=[label_dict[x] for x in labels], hue_order=h_order)\n",
    "#     plt.xlabel(\"PC1\")\n",
    "#     plt.ylabel(\"PC2\")\n",
    "#     plt.title(\"PCA of Embedding Weights\")\n",
    "    plt.xlabel(\"Embedding 1\", fontsize=20)\n",
    "    plt.ylabel(\"Embedding 2\", fontsize=20)\n",
    "    plt.title(\"Embedding Weights - VAE Model\", fontsize=20)\n",
    "    plt.legend(fontsize=20)\n",
    "    if annotations is not None:\n",
    "        for line in range(len(labels)):\n",
    "#             if weights_pca[line,1] > -0.3:\n",
    "#             if annotations[line] != \"Mozabite\":\n",
    "#                 continue\n",
    "            scplot.text(embeddings[line,0]+0.001, embeddings[line,1], \n",
    "                         annotations[line], horizontalalignment='left', color='black', size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 15], dpi=300)\n",
    "sp_full = [eval_dataset.suppop_label_index[x] for x in suppop_labels]\n",
    "p_full = [eval_dataset.pop_label_index[x] for x in pop_labels]\n",
    "plot_weights_pca(embeddings, sp_full, annotations=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce, Plot, and Evaluate Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_tree(model, embeddings, device, fast_decoding):\n",
    "    \"\"\"Build a binary tree (nx graph) from leaves' embeddings. Assume points are normalized to same radius.\n",
    "        Taken from HypHC repo (https://github.com/HazyResearch/HypHC)\n",
    "    \n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        leaves_embeddings = model.HypHCLoss.normalize_embeddings(torch.tensor(embeddings).to(device))\n",
    "        leaves_embeddings = project(leaves_embeddings).cpu()\n",
    "    sim_fn = lambda x, y: torch.sum(x * y, dim=-1)\n",
    "    if fast_decoding:\n",
    "        parents = nn_merge_uf_fast_np(leaves_embeddings, S=sim_fn, partition_ratio=1.2)\n",
    "    else:\n",
    "        parents = sl_from_embeddings(leaves_embeddings, sim_fn)\n",
    "    tree = nx.DiGraph()\n",
    "    for i, j in enumerate(parents[:-1]):\n",
    "        tree.add_edge(j, i)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot tree\n",
    "embs_tree = embeddings\n",
    "tree = decode_tree(model, embs_tree, device, True)\n",
    "with torch.no_grad():\n",
    "    embs_normed = model.HypHCLoss.normalize_embeddings(torch.tensor(embs_tree).to(device))\n",
    "    leaves_embeddings = project(embs_normed).cpu().numpy()\n",
    "fig = plt.figure(figsize=(15, 15), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax = plot_tree_from_leaves(ax, tree, leaves_embeddings * 100, labels=sp_full)\n",
    "\n",
    "# Uncomment to add subpopulation labels - but it's very crowded\n",
    "# to_write = [np.random.choice([True, False], p=[0.1, 0.9]) for x in range(len(suppop_labels))]\n",
    "# for line in range(len(suppop_labels)):\n",
    "#     if to_write[line]:\n",
    "#         ax.text(leaves_embeddings[line,0] * 100 +0.001, leaves_embeddings[line,1] * 100, \n",
    "#                      p_full[line], ha='left', \n",
    "#                 color='black', va=\"baseline\", rotation=0, size=6)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pairwise_similarities(data, sim_func):\n",
    "    '''\n",
    "    Creates a similarity matrix from the SNP data using the specified similarity function\n",
    "    This is a numpy version of the function used in training\n",
    "    '''\n",
    "    sim_matrix = np.zeros((data.shape[0], data.shape[0]))\n",
    "    #Fill in matrix\n",
    "    for ind in range(data.shape[0]):\n",
    "        for ind2 in range(data.shape[0]):\n",
    "            sim_matrix[ind][ind2] = sim_func(data[ind], data[ind2])\n",
    "    #Divide by maximum for normalization\n",
    "    sim_matrix /= np.amax(sim_matrix)\n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "\n",
    "def make_pairwise_label_similarities(data):\n",
    "    sim_matrix = np.zeros((data.shape[0], data.shape[0]))\n",
    "    #Fill in matrix\n",
    "    for ind in range(data.shape[0]):\n",
    "        for ind2 in range(data.shape[0]):\n",
    "            sim_matrix[ind][ind2] = (int(p_full[ind] == p_full[ind2]) + int(sp_full[ind] == sp_full[ind2])) // 2\n",
    "    #Divide by maximum for normalization\n",
    "    sim_matrix /= np.amax(sim_matrix)\n",
    "    return sim_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_mat = make_pairwise_similarities(eval_dataset.snps, lambda x,y: (np.sum(x==y)) / len(x))\n",
    "# sim_mat = make_pairwise_similarities(eval_snps_old, lambda x,y: (np.sum(x==y)) / len(x))\n",
    "\n",
    "# sim_mat = make_pairwise_label_similarities(eval_dataset.snps)\n",
    "# train_sim_mat = np.load(\"/scratch/users/patelas/hyperLAI/snp_data/whole_genome/variance_filtered_500000_updated/sim_mats/train_hamming.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sim_mat_minmax = (sim_mat - 0.451) / 0.549\n",
    "# sim_mat_minmax = np.where(sim_mat_minmax >= 0, sim_mat_minmax, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.min(sim_mat_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates Dasgupta's cost of the tree\n",
    "print(dasgupta_cost(tree, sim_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"/scratch/users/patelas/hyperLAI/snp_data/whole_genome/variance_filtered_500000_updated/sim_mats/train_hamming.npy\", sim_mat)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test Reconstruction Ability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part does a few different things, all relating to the differences between true and reconstructed data. It also runs PCA and inverse PCA for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Some basic stats of the differences between true and reconstructed data\n",
    "abs_diffs = np.absolute(snps-recons)\n",
    "print(np.min(abs_diffs), np.mean(abs_diffs), np.max(abs_diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reconstruction accuracy\n",
    "preds = recons > 0.5\n",
    "abs_preds = np.sum(snps == preds) / len(snps.flatten())\n",
    "print(abs_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(sorted(np.mean(np.absolute(snps-recons),axis=1)))\n",
    "plt.show()\n",
    "plt.plot(sorted(np.min(np.absolute(snps-recons),axis=1)))\n",
    "plt.show()\n",
    "plt.plot(sorted(np.max(np.absolute(snps-recons),axis=1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compare reconstruction performance to reconstruction performance of PCA\n",
    "recon_pca = PCA(n_components=100)\n",
    "recon_pca.fit(dataset.snps[train_indices])\n",
    "comps = recon_pca.transform(snps)\n",
    "pca_reprod = recon_pca.inverse_transform(comps)\n",
    "reprod_preds = pca_reprod > 0.5\n",
    "print(np.sum(snps == reprod_preds) / len(snps.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Fidelity of Reproduced Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part takes synthetic embeddings (designed to reflect new \"samples\") and then runs them through the decoder and then the encoder. We then observe how the new embeddings reflect the original embedding space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_acc_test(model, embeddings):\n",
    "    '''\n",
    "    Runs reconstruction and then re-embeds \n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recons = model.generate(embeddings.to(device))\n",
    "        new_embs = model.embed(recons)\n",
    "    return new_embs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We produce \"fake embeddings\" by adding random noise to current embeddings\n",
    "fake_embs = embeddings + np.random.normal(0, 0.05, size=embeddings.shape)\n",
    "#We then run them through the decoder and encoder\n",
    "test_emb = torch.tensor(fake_embs).float()\n",
    "recon_embs = decoder_acc_test(model, test_emb).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Take a look at some summary stats \n",
    "euc_dists = np.sqrt(np.sum((recon_embs - fake_embs)**2, axis=1))\n",
    "print(np.min(euc_dists), np.mean(euc_dists), np.max(euc_dists))\n",
    "axis_dists = np.absolute(recon_embs - fake_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Look at pattern of new embeddings and how it compares to the true pattern\n",
    "plot_weights_pca(recon_embs, sp_full)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Quality of Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part reconstructs data from embeddings, and then finds the minimum L1 distance between each reconstructed genotype and any true genotype. The embeddings provided can be either real or synthetic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_recons(model, embeddings):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recons = model.generate(torch.tensor(embeddings).float().to(device))\n",
    "    return recons.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1_norm_diff(recon, real_snps):\n",
    "    diffs = np.sum(np.abs(real_snps - (recon > 0.5).astype(float)), axis=1)\n",
    "    return np.min(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_to_use = np.random.choice(len(embeddings), 20, replace=False)\n",
    "fake_embs = embeddings[rows_to_use] + np.random.normal(0, 0.05, size=embeddings[rows_to_use].shape)\n",
    "# fake_embs = embeddings[rows_to_use]\n",
    "recons = produce_recons(model, fake_embs).numpy()\n",
    "min_diffs = np.apply_along_axis(lambda x: l1_norm_diff(x, snps), 1, recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dasgupta's Cost Using Regular Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test set Dasgupta's costs\n",
    "#Single: 68384691.04029198\n",
    "#Complete: 68183619.215324\n",
    "#Average: 68071384.105212\n",
    "#Weighted: 68074478.67049602\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hamming_dist = lambda x, y: (np.sum(x != y) / len(x)) #Same as \"hamming\" in scipy linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_hc(data, metric, method):\n",
    "    hc_tree = nx.DiGraph()\n",
    "    linkage_list = sch.linkage(data, metric=metric, method=method)\n",
    "    for snp in range(len(data)):\n",
    "        hc_tree.add_node(snp)\n",
    "    for ind, extra_clust in enumerate(linkage_list):\n",
    "        hc_tree.add_node(len(data) + ind)\n",
    "        hc_tree.add_edge(len(data) + ind, int(extra_clust[0]))\n",
    "        hc_tree.add_edge(len(data) + ind, int(extra_clust[1]))\n",
    "    return hc_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hc_tree = perform_hc(eval_dataset.snps, \"hamming\", \"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hc_tree, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(dasgupta_cost(hc_tree, sim_mat), dasgupta_cost(tree, sim_mat))\n",
    "print(dasgupta_cost(hc_tree, sim_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_array = np.array([[1,1,1,1], [1,1,1,0], [0,0,0,0], [1,0,0,0]])\n",
    "test_tree = perform_hc(test_array, \"hamming\", \"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tree.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(tree) == type(hc_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(hc_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test that \"tree\" does not have any true nodes as the start of edges\n",
    "tree_edges_start = [x[0] for x in tree.edges]\n",
    "assert min(tree_edges_start) == len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test that \"hc_tree\" does not have any true nodes as the start of edges\n",
    "hc_tree_edges_start = [x[0] for x in hc_tree.edges]\n",
    "assert min(hc_tree_edges_start) == len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test that both trees have the same number of edges\n",
    "assert len(tree.edges) == len(hc_tree.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assert that all true nodes are found as the recipient of edges in both trees\n",
    "tree_edges_end = [x[1] for x in tree.edges]\n",
    "tree_all_nodes = [x not in tree_edges_end for x in range(len(eval_dataset))]\n",
    "assert sum(tree_all_nodes) == 0\n",
    "hc_tree_edges_end = [x[1] for x in hc_tree.edges]\n",
    "hc_tree_all_nodes = [x not in hc_tree_edges_end for x in range(len(eval_dataset))]\n",
    "assert sum(hc_tree_all_nodes) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Assert that no more than two edges start from the same point\n",
    "tree_edges_start = [x[0] for x in tree.edges]\n",
    "tree_counts = [tree_edges_start.count(x) for x in tree_edges_start]\n",
    "assert max(tree_counts) <= 2\n",
    "hc_tree_edges_start = [x[0] for x in hc_tree.edges]\n",
    "hc_tree_counts = [hc_tree_edges_start.count(x) for x in hc_tree_edges_start]\n",
    "assert max(hc_tree_counts) <= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Cross-Prediction Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcosts = np.array([68103647.032476,\n",
    "68091529.558308,\n",
    "68089696.80722,\n",
    "68092794.567008,\n",
    "68192833.336424,\n",
    "68093968.794136,\n",
    "68165172.798208,\n",
    "68197330.18846399,\n",
    "68091692.73715201,\n",
    "68198013.69252])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(dcosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.std(dcosts) * 1.96 * 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperLAI-env",
   "language": "python",
   "name": "hyperlai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
