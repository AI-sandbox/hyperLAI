{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import unittest\n",
    "import numpy as np\n",
    "import torch\n",
    "sys.path.append(\"../../hyperLAI\")\n",
    "sys.path.append(\"../../../libraries\")\n",
    "from utils.sim_funcs import sim_func_dict\n",
    "from utils.model_utils import *\n",
    "from utils.generate_dataset import *\n",
    "from features.hyperLAIdataset import HyperLoader\n",
    "from models.fc_model import fc_model\n",
    "from utils.sim_funcs import sim_func_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inds = np.load(\"/scratch/users/patelas/hyperLAI/ancestry_training_splits/80_10_10/train_indices.npy\")\n",
    "valid_inds = np.load(\"/scratch/users/patelas/hyperLAI/ancestry_training_splits/80_10_10/valid_indices.npy\")\n",
    "test_inds = np.load(\"/scratch/users/patelas/hyperLAI/ancestry_training_splits/80_10_10/test_indices.npy\")\n",
    "all_inds = np.sort(np.concatenate([train_inds, valid_inds, test_inds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"/scratch/users/patelas/hyperLAI/snp_data/\"\n",
    "chrom = 22\n",
    "#Dataset with all samples and regions\n",
    "dataset = HyperLoader(data_dir, all_inds, [0,1,2,3,4,5,6], 22)\n",
    "chr22_data = load_dataset(data_dir + \"ref_final_beagle_phased_1kg_hgdp_sgdp_chr%s_hg19.vcf.gz\"%(chrom), \n",
    "                                     data_dir + \"reference_panel_metadata.tsv\", \"./\", chromosome=chrom, \n",
    "                                     verbose=True, filter_admixed=True, filter_missing_coord=True)\n",
    "#Dataset with all samples for regions 0,1,2\n",
    "dataset_restricted = HyperLoader(data_dir, all_inds, [0,1,2], 22)\n",
    "#Dataset with test samples for regions 0,1,2\n",
    "dataset_restricted_test = HyperLoader(data_dir, test_inds, [0,1,2], 22)\n",
    "#Dataset with all test samples\n",
    "dataset_all_test = HyperLoader(data_dir, test_inds, [0,1,2,3,4,5,6], 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset2 = HyperLoader(\"/scratch/users/patelas/hyperLAI/snp_data/whole_genome/variance_filtered_500000/\", [0,1,2,3,4,5,6], \"all\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestDataLoader(unittest.TestCase):\n",
    "    def test_correct_pos(self):\n",
    "        '''\n",
    "        Tests that the indices of the data in the HyperLoader class match what they should\n",
    "        Specifically, in comparison to the indices of load_dataset\n",
    "        Since each individual has 2 haplotypes, index 12 of the dataloader should correspond to the start of the individual at index 6 from load_dataset\n",
    "        '''\n",
    "        #Extract the first and second sets of chromosomes for individual 6 (just first 5 snps)\n",
    "        chr22_snps_first = chr22_data[0][0:5,6,0]\n",
    "        chr22_snps_second = chr22_data[0][0:5,6,1]\n",
    "        #Extract what should be the corresponding sets of SNPs\n",
    "        dataset_snps_first = dataset[12][0][0:5].numpy()\n",
    "        dataset_snps_second = dataset[13][0][0:5].numpy()\n",
    "        #Ensure the SNPs are equal\n",
    "        self.assertEqual(np.sum(chr22_snps_first == dataset_snps_first), 5,\n",
    "                        \"Order between dataloader and initial data is wrong\")\n",
    "        self.assertEqual(np.sum(chr22_snps_second == dataset_snps_second), 5,\n",
    "                        \"Order between dataloader and initial data is wrong for second haplotype\")\n",
    "        #Ensure the suppop and pop labels match (only for index 12, we will do 13 in the next test)\n",
    "        self.assertEqual(chr22_data[1][6], dataset[12][2],\n",
    "                        \"Superpopulation labels don't match\")\n",
    "        self.assertEqual(chr22_data[2][6], dataset[12][1],\n",
    "                        \"Population labels don't match\")\n",
    "    def test_correct_repeat(self):\n",
    "        '''\n",
    "        Both indices 12 come from the same individual, so the labels should match. \n",
    "        Checks this is the case. \n",
    "        '''\n",
    "        #Ensure labels are same for both sets of chromosomes for one individual\n",
    "        self.assertEqual(dataset[12][1], dataset[13][1],\n",
    "                        \"Superpopulation labels don't match between the same individual\")\n",
    "        self.assertEqual(dataset[12][2], dataset[13][2],\n",
    "                        \"Population labels don't match between the same individual\")\n",
    "        #Do same for another individual\n",
    "        self.assertEqual(dataset[1256][1], dataset[1257][1],\n",
    "                        \"Superpopulation labels don't match between the same individual\")\n",
    "        self.assertEqual(dataset[1256][2], dataset[1257][2],\n",
    "                        \"Population labels don't match between the same individual\")\n",
    "    def test_restricted(self):\n",
    "        '''\n",
    "        Test that restricting to certain continents works\n",
    "        '''\n",
    "        #Make sure no unwanted labels are there\n",
    "        self.assertEqual(np.sum(np.isin(dataset_restricted.suppop_labels, [3,4,5,6])), 0,\n",
    "                        \"Contains labels from individuals that shouldn't be there\")\n",
    "        #Make sure SNP data has same size as label data\n",
    "        self.assertEqual(dataset_restricted.snps.shape[0], dataset_restricted.suppop_labels.shape[0],\n",
    "                        \"Individuals were not subsetted properly\")\n",
    "    def test_restrict_indices(self):\n",
    "        #Make sure restricting to test indices keeps the correct labels\n",
    "        self.assertEqual(np.sum(np.sort(dataset.pop_labels[test_inds]) != np.sort(dataset_all_test.pop_labels)), 0,\n",
    "                        \"Test dataset does not have the correct labels as expected from examining the entire dataset\")\n",
    "        #Make sure restricting to data split + continental population works well too\n",
    "        cont_from_all_test = dataset_all_test.pop_labels[np.where(np.isin(dataset_all_test.suppop_labels, [0,1,2]))]\n",
    "        cont_from_restricted_test = dataset_restricted_test.pop_labels\n",
    "        self.assertEqual(np.sum(np.sort(cont_from_all_test) != np.sort(cont_from_restricted_test)), 0,\n",
    "                        \"Test dataset with only populations 0,1,2 does not have the correct labels as expected from\\\n",
    "                        examining the test dataset for the whole population\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = fc_model(1000, 3, [50,40,30], 20, \n",
    "                     [0.1, 0.2, 0.1], 0.01, 1e-3, 1e-2, 0.999)\n",
    "\n",
    "print(model)\n",
    "print(model.HypLoss.temperature)\n",
    "print(model.HypLoss.init_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class test_utils(unittest.TestCase):\n",
    "    '''\n",
    "    Testing various utilities that are important in the training process\n",
    "    '''\n",
    "    def test_triple_ids(self):\n",
    "        '''\n",
    "        Testing that for a total of 7 elements,\n",
    "        generate_triple_ids produces 35 unique elements\n",
    "        '''\n",
    "        ids = generate_triple_ids(7)\n",
    "        self.assertEqual(len(ids), 35)\n",
    "        self.assertEqual(len(np.unique(ids, axis=0)), 35)\n",
    "    def test_pairwise_similarities(self):\n",
    "        '''\n",
    "        Ensures that the pairwise similarities are calculated correctly\n",
    "        This also indirectly tests the correctness of the hamming distance function\n",
    "        '''\n",
    "        #Create fake \"SNP data\"\n",
    "        dummy_data = torch.tensor([[0,0,0,1],[1,0,0,1],[1,1,1,1]])\n",
    "        #Calculate similarity matrix using function\n",
    "        sim_mat = make_pairwise_similarities(dummy_data, sim_func_dict[\"hamming\"])\n",
    "        #Expected result\n",
    "        exp_result = np.array([[1, 0.75, 0.25], [0.75, 1, 0.5], [0.25, 0.5, 1]])\n",
    "        #Determine if the two are equal\n",
    "        self.assertEqual(np.sum(sim_mat.numpy() != exp_result), 0)\n",
    "    def test_trips_and_sims(self):\n",
    "        '''\n",
    "        Tests that the trips_and_sims function works as it should\n",
    "        '''\n",
    "        #Fake data\n",
    "        dummy_data = torch.tensor([[0,0,0,1],[1,0,0,1],[1,1,1,1], [0,0,0,0], [0,1,1,0]])\n",
    "        #Produce similarity matrix (already tested)\n",
    "        sim_mat = make_pairwise_similarities(dummy_data, sim_func_dict[\"hamming\"])\n",
    "        #Expected triples to be produced (already tested)\n",
    "        trips_exp = generate_triple_ids(5)\n",
    "        #Produce triples and similarities\n",
    "        trips, sims = trips_and_sims(dummy_data, sim_func_dict[\"hamming\"])\n",
    "        #Assert trips are equal\n",
    "        self.assertEqual(np.sum(np.array(trips_exp) != trips.numpy()), 0)\n",
    "        #What the last set of three similarities is expected to be\n",
    "        exp_last_sim = np.array([sim_mat[2,3], sim_mat[2,4], sim_mat[3,4]])\n",
    "        #Assert sims are equal\n",
    "        self.assertEqual(np.sum(exp_last_sim != sims.numpy()[-1]), 0)\n",
    "    def test_train_test_split(self):\n",
    "        '''\n",
    "        Makes sure the results of the train-test split are reproducible\n",
    "        '''\n",
    "        #Run it twice\n",
    "        tr1, va1, te1 = train_valid_test(10, 0.8, 0.1)\n",
    "        tr2, va2, te2 = train_valid_test(10, 0.8, 0.1)\n",
    "        #Assert valid and test and train are equal for both runs\n",
    "        self.assertEqual(va1, va2)\n",
    "        self.assertEqual(te1, te2)\n",
    "        self.assertEqual(np.sum(tr1 != tr2), 0)\n",
    "        #Assert no overlaps\n",
    "        tr_large, val_large, test_large = train_valid_test(1000, 0.8, 0.1)\n",
    "        self.assertEqual(len(set(tr_large).intersection(set(val_large))), 0)\n",
    "        self.assertEqual(len(set(tr_large).intersection(set(test_large))), 0)\n",
    "        self.assertEqual(len(set(val_large).intersection(set(test_large))), 0)\n",
    "        self.assertEqual(len(tr_large) + len(val_large) + len(test_large), 1000)\n",
    "    def test_variance(self):\n",
    "        '''\n",
    "        Tests that the variancefilter works properly\n",
    "        '''\n",
    "        class DummyLoader():\n",
    "            def __init__(self, dummy_data):\n",
    "                self.snps = dummy_data\n",
    "        #Define fake data and dataloader\n",
    "        fake_snps = np.array([[1,1,1,1,0,0,0,0], [1,0,1,0,1,1,1,1], [1,0,0,1,0,0,0,0], [0,0,0,0,1,1,1,1]]).T\n",
    "        dummy_data1 = DummyLoader(fake_snps)        \n",
    "        dummy_data2 = DummyLoader(fake_snps)\n",
    "        #Define two different sets of train indices, to make sure it only calculates on the train data\n",
    "        variance_filter(dummy_data1, [0,1,2,3], 2)\n",
    "        variance_filter(dummy_data2, [0,1,2,3,4,5,6,7], 2)\n",
    "        #Ensure both are correct\n",
    "        self.assertEqual(np.sum(dummy_data1.snps != fake_snps[:,[2,1]]), 0)\n",
    "        self.assertEqual(np.sum(dummy_data2.snps != fake_snps[:,[3,0]]), 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = generate_triple_ids(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_data= DummyLoader(np.array([[1,1,1,1,0,0,0,0], [1,0,1,0,1,1,1,1], [1,0,0,1,0,0,0,0], [0,0,0,0,1,1,1,1]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_filter(dummy_data, [0,1,2,3], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyLoader():\n",
    "    def __init__(self, dummy_data):\n",
    "        self.snps = dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data.snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperLAI-env",
   "language": "python",
   "name": "hyperlai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
